## Identification and classification of aortic dissection in 3D CT scans

This repository contains the code and implementation steps for our work related to identifying and classifying aortic dissection (AD) cases in 3D CT scans. We leveraged the _nnUNet v2_ model to perform automatic segmentation of the aorta (**four labels**) in CT scans. After retrieving the segmentation masks, a simple yet effective procedure was used for patient-level classification (**three classes**) based on the modelâ€™s results (segment presence and Dice score threshold). The two types of AD, as per _Stanford criteria_, are **Type A (TBAD) and Type B (TBAD)** (refer to the attached figure _AD classes.pdf_). The third class we are considering here is a normal CT scan, dubbed **NoAD**.

The dataset used for experimentation was collected by retrospectively searching the picture archiving and communication system (PACS) database based at a single regional tertiary vascular and cardiothoracic unit, following national and local ethics approval. Contrast-enhanced CTA scans were included and reviewed by radiologists. The data annotation is performed by experts (radiologists) and consists of four labels: **dissected pre, dissected post, non pre, and non post**, where _pre_ and _post_ refer to the location of the dissection relative to the subclavian artery, a branch of the aorta acting as a boundary between _TAAD_ and _TBAD_, respectively. A scan containing the label _dissected pre_ is classified as _TAAD_; if it contains _dissected post_ without _dissected pre_, it is classified as _TBAD_, and if it contains _non pre_ and _non post_ labels only, then the scan is classified as _NoAD_.

### Implementation procedure (Figure _flow diagram.pdf_)
#### Step 1: Segmenting four aorta labels
We used the nnUNet v2 segmentation model to segment the different sections of the aorta according to the labels. Convert the scans and masks to the appropriate NIfTI format, if required, to provide input to the nnUNet model. Follow the installation and usage instructions provided in the [GitHub repo](https://github.com/MIC-DKFZ/nnUNet) to utilise the nnUNet model. Initially, we have a set of CT scans and corresponding masks (comprising five classes: four labels and one _background_ class). They are provided as input to the nnUNet model to train it for segmenting the five labels. nnUNet creates five data splits (folds) during the data preparation phase. We can choose any one of the folds (by specifying the fold number 0, 1, 2, 3, 4 in the training command) for validation, and the rest of the data is used for training. All the folds can also be used for training without any validation data (where the fold is set as 'all' while training). In this study, three nnUNet v2 models are trained and tested using different configurations: a low-resolution model (**3d lowres with fold=0**), a full-resolution model (**3d fullres with fold=0**) and a full-resolution model trained on the entire dataset without separate validation data, or validation is also performed on training data (**3d fullres with fold='all'**). We are referring to this model as **3d fullres fall** to differentiate it from **3d fullres**. More information about the model and the configurations can be found in [1](https://www.nature.com/articles/s41592-020-01008-z) and [2](https://link.springer.com/chapter/10.1007/978-3-031-72114-4_47). All three configurations are trained on our dataset for 100 epochs. We used the best checkpoints (**checkpoint best.pth**) for the test data predictions. The default nnUNet parameters are used for preprocessing, training, and prediction, except for the number of processes, which were reduced to run the model effectively on our GPUs.

#### Step 2: Classifying the scans in three classes (TAAD, TBAD, NoAD)
Use the attached _patient-level classify.ipynb_ jupyter notebook for classification. You need to provide the path to the **summary** JSON file (generated by the trained nnUNet model for the test data, and it contains the dice scores, other segmentation metrics, and predicted masks info) in the notebook. This file will be saved under the folder **nnUNet >> data >> nnUNet_results >> Dataset**. As per the classification criteria set in the notebook, you will be able to retrieve a class (out of three classes) corresponding to each of the test scans. 


For more information regarding this work, refer to the article below and cite it.

Kaur, P., Asif, A., Lakshminarayan, R., Monekosso, D., & Remagnino, P. (2026). Identification and classification of aortic dissection in 3D CT scans. Zenodo. https://doi.org/10.5281/zenodo.18303099
